{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np \nimport pandas as pd \nimport os\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nfrom tqdm.notebook import tqdm\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"if torch.cuda.is_available():\n    print(\"GPU available\")\n    device = \"cuda:0\"\nelse:\n    print(\"CPU available\")\n    device = \"cpu\"\n    \ndevice = torch.device(device)\nprint(device)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = os.path.join(\"/kaggle\", \"input\", \"cityscapes-image-pairs\", \"cityscapes_data\")\ntrain_dir = os.path.join(data_dir, \"train\") \nval_dir = os.path.join(data_dir, \"val\")\ntrain_fns = os.listdir(train_dir)\nval_fns = os.listdir(val_dir)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train patch size:\", len(train_fns))\nprint(\"validation patch size:\", len(val_fns))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_image_fp = os.path.join(train_dir, train_fns[10])\nsample_image = Image.open(sample_image_fp)\nplt.imshow(sample_image)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_image(image):\n    image = np.array(image)\n    cityscape = image[:, :256, :] \n    label = image[:, 256:, :]\n    return cityscape, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_image = np.array(sample_image)\nprint(sample_image.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cityscape, label = split_image(sample_image)\nprint(\"image shape:\" , cityscape.shape)\nprint(\"label shape:\" , label.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cityscape = Image.fromarray(cityscape) \nlabel = Image.fromarray(label)\n\nfig, axes = plt.subplots(1, 2, figsize=(5, 5))\naxes[1].imshow(cityscape)\naxes[0].imshow(label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_items = 1000\ncolor_array = np.random.choice(range(256), 3*num_items).reshape(-1, 3)\nprint(color_array.shape)\nprint(color_array[:5, :])\n\nnum_classes = 10\nlabel_model = KMeans(n_clusters=num_classes)\nlabel_model.fit(color_array)\n\nlabel_model.predict(color_array[:5, :])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cityscape, label = split_image(sample_image)\nlabel_class = label_model.predict(label.reshape(-1, 3)).reshape(256, 256)\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\naxes[0].imshow(cityscape)\naxes[1].imshow(label)\naxes[2].imshow(label_class)\nprint(label_class)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CityscapeDataset(Dataset):\n    \n    def __init__(self, image_dir, label_model):\n        self.image_dir = image_dir\n        self.image_fns = os.listdir(image_dir)\n        self.label_model = label_model\n        \n    def __len__(self):\n        return len(self.image_fns)\n    \n    def __getitem__(self, index):\n        image_fn = self.image_fns[index]\n        image_fp = os.path.join(self.image_dir, image_fn)\n        image = Image.open(image_fp).convert('RGB')\n        image = np.array(image)\n        cityscape, label = self.split_image(image)\n        label_class = self.label_model.predict(label.reshape(-1, 3)).reshape(256, 256)\n        cityscape = self.transform(cityscape)\n        label_class = torch.Tensor(label_class).long()\n        return cityscape, label_class\n    \n    def split_image(self, image):\n        image = np.array(image)\n        cityscape, label = image[:, :256, :], image[:, 256:, :]\n        return cityscape, label\n    \n    def transform(self, image):\n        transform_ops = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n        ])\n        return transform_ops(image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = CityscapeDataset(train_dir, label_model)\ncityscape, label_class = dataset[0]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class UNet(nn.Module):\n    \n    def __init__(self, num_classes):\n        super(UNet, self).__init__()\n        self.num_classes = num_classes\n        self.contracting_11 = self.conv_block(in_channels=3, out_channels=64)\n        self.contracting_12 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.contracting_21 = self.conv_block(in_channels=64, out_channels=128)\n        self.contracting_22 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.contracting_31 = self.conv_block(in_channels=128, out_channels=256)\n        self.contracting_32 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.contracting_41 = self.conv_block(in_channels=256, out_channels=512)\n        self.contracting_42 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.middle = self.conv_block(in_channels=512, out_channels=1024)\n        self.expansive_11 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_12 = self.conv_block(in_channels=1024, out_channels=512)\n        self.expansive_21 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_22 = self.conv_block(in_channels=512, out_channels=256)\n        self.expansive_31 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_32 = self.conv_block(in_channels=256, out_channels=128)\n        self.expansive_41 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_42 = self.conv_block(in_channels=128, out_channels=64)\n        self.output = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=3, stride=1, padding=1)\n        \n    def conv_block(self, in_channels, out_channels):\n        block = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n                                    nn.ReLU(),\n                                    nn.BatchNorm2d(num_features=out_channels),\n                                    nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n                                    nn.ReLU(),\n                                    nn.BatchNorm2d(num_features=out_channels))\n        return block\n    \n    def forward(self, X):\n        contracting_11_out = self.contracting_11(X) \n        contracting_12_out = self.contracting_12(contracting_11_out) \n        contracting_21_out = self.contracting_21(contracting_12_out) \n        contracting_22_out = self.contracting_22(contracting_21_out) \n        contracting_31_out = self.contracting_31(contracting_22_out) \n        contracting_32_out = self.contracting_32(contracting_31_out) \n        contracting_41_out = self.contracting_41(contracting_32_out) \n        contracting_42_out = self.contracting_42(contracting_41_out) \n        middle_out = self.middle(contracting_42_out) \n        expansive_11_out = self.expansive_11(middle_out) \n        expansive_12_out = self.expansive_12(torch.cat((expansive_11_out, contracting_41_out), dim=1))\n        expansive_21_out = self.expansive_21(expansive_12_out) \n        expansive_22_out = self.expansive_22(torch.cat((expansive_21_out, contracting_31_out), dim=1))\n        expansive_31_out = self.expansive_31(expansive_22_out) \n        expansive_32_out = self.expansive_32(torch.cat((expansive_31_out, contracting_21_out), dim=1))\n        expansive_41_out = self.expansive_41(expansive_32_out) \n        expansive_42_out = self.expansive_42(torch.cat((expansive_41_out, contracting_11_out), dim=1)) \n        output_out = self.output(expansive_42_out) \n        return output_out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = UNet(num_classes=num_classes)\nprint(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_loader = DataLoader(dataset, batch_size=4)\nprint(len(dataset), len(data_loader))\n\nX, Y = iter(data_loader).next()\nprint(X.shape, Y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred = model(X)\nprint(Y_pred.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 16\n\nepochs = 100\nlr = 0.001","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = CityscapeDataset(train_dir, label_model)\ndata_loader = DataLoader(dataset, batch_size=batch_size)\n\nmodel = UNet(num_classes=num_classes).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"step_losses = []\nepoch_losses = []\nfor epoch in tqdm(range(epochs)):\n    epoch_loss = 0\n    for X, Y in tqdm(data_loader, total=len(data_loader), leave=False):\n        X, Y = X.to(device), Y.to(device)\n        optimizer.zero_grad()\n        Y_pred = model(X)\n        loss = criterion(Y_pred, Y)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n        step_losses.append(loss.item())\n    epoch_losses.append(epoch_loss/len(data_loader))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(10, 5))\naxes[0].plot(step_losses)\naxes[1].plot(epoch_losses)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name = \"U-Net.pth\"\ntorch.save(model.state_dict(), model_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_path = \"/kaggle/working/U-Net.pth\"\nmodel_ = UNet(num_classes=num_classes).to(device)\nmodel_.load_state_dict(torch.load(model_path))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_batch_size = 8\ndataset = CityscapeDataset(val_dir, label_model)\ndata_loader = DataLoader(dataset, batch_size=test_batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, Y = next(iter(data_loader))\nX, Y = X.to(device), Y.to(device)\nY_pred = model_(X)\nprint(Y_pred.shape)\nY_pred = torch.argmax(Y_pred, dim=1)\nprint(Y_pred.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inverse_transform = transforms.Compose([\n    transforms.Normalize((-0.485/0.229, -0.456/0.224, -0.406/0.225), (1/0.229, 1/0.224, 1/0.225))\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(test_batch_size, 3, figsize=(3*5, test_batch_size*5))\n\nfor i in range(test_batch_size):\n    \n    landscape = inverse_transform(X[i]).permute(1, 2, 0).cpu().detach().numpy()\n    label_class = Y[i].cpu().detach().numpy()\n    label_class_predicted = Y_pred[i].cpu().detach().numpy()\n    \n    axes[i, 0].imshow(landscape)\n    axes[i, 0].set_title(\"Landscape\")\n    axes[i, 1].imshow(label_class)\n    axes[i, 1].set_title(\"Label Class\")\n    axes[i, 2].imshow(label_class_predicted)\n    axes[i, 2].set_title(\"Label Class - Predicted\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}